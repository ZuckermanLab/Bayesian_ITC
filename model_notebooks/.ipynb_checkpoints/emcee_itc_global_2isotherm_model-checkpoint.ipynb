{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import emcee \n",
    "import itcfunctions as itc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "##some params that cannot easily be read\n",
    "##while Kb is unlikely to change, T and V0 will be instrument/experiment dependent\n",
    "#boltzmann constant\n",
    "Kb = 0.001987  \n",
    "#Temperature\n",
    "T = 273.15+25  \n",
    "#initial volume in ITC cell, L units\n",
    "V0 = 1.42e-3   \n",
    "itc_constants = [Kb,T,V0]\n",
    "\n",
    "\n",
    "\n",
    "##simple function for reading CSV file of injection volumes and integrated heats\n",
    "#converts from expected values in uL (injection volume) and ucal -- default outputs from table in origin \n",
    "#see included example file for formatting.\n",
    "def get_data(file_name):\n",
    "    inj_list = np.empty(shape=(0))\n",
    "    dq_list = np.empty(shape=(0))\n",
    "    with open(file_name+'.csv') as F:\n",
    "        for line in F:\n",
    "            values = line.strip('\\n').split(',')\n",
    "            if len(values) == 2:\n",
    "                dq_list = np.append(dq_list,float(values[0]))\n",
    "                inj_list = np.append(inj_list,float(values[1])*1e-6)\n",
    "    return [inj_list],dq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12 parameter itc model sampling\n",
    "def log_likelihood(theta, y_obs, extra_parameters):\n",
    "    '''log of Guassian likelihood distribution'''\n",
    "    curr_sigma = theta[-2]\n",
    "    curr_sigma_2 = theta[-1]\n",
    "    \n",
    "    #calc y_pred for each isotherm\n",
    "    y_pred_1 = itc.get_dq_list(theta[0],theta[1],theta[2],theta[3],theta[4],theta[6],theta[8],extra_parameters[0],itc_constants)\n",
    "    y_pred_2 = itc.get_dq_list(theta[0],theta[1],theta[2],theta[3],theta[5],theta[7],theta[9],extra_parameters[1],itc_constants)\n",
    "    y_pred = np.concatenate((y_pred_1,y_pred_2))\n",
    "    \n",
    "    # calculate normal log likelihood\n",
    "    logl = -len(y_obs[0]) * np.log(np.sqrt(2.0 * np.pi) * curr_sigma)\n",
    "    logl += -np.sum((y_obs[0] - y_pred_1) ** 2.0) / (2.0 * curr_sigma ** 2.0) \n",
    "    logl += -len(y_obs[1]) * np.log(np.sqrt(2.0 * np.pi) * curr_sigma_2)\n",
    "    logl += -np.sum((y_obs[1] - y_pred_2) ** 2.0) / (2.0 * curr_sigma_2 ** 2.0) \n",
    "    return logl\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    '''log of uniform prior distribution'''\n",
    "    \n",
    "    #restate ref\n",
    "    pt_true = 0.0004\n",
    "    pt_true_2 = 0.0004\n",
    "    lt_true = 1.2e-05\n",
    "    lt_true_2 = 1.45e-05\n",
    "\n",
    "    #unpack theta\n",
    "    dg = theta[0]\n",
    "    ddg = theta[1]\n",
    "    dh = theta[2]\n",
    "    ddh = theta[3]\n",
    "    pt = theta[4]\n",
    "    pt_2 = theta[5]\n",
    "    lt = theta[6]\n",
    "    lt_2 = theta[7]\n",
    "    dh_0 = theta[8]\n",
    "    dh_0_2 = theta[9]\n",
    "    sigma = theta[-2]\n",
    "    sigma_2 = theta[-1]\n",
    "\n",
    "    # if prior is between boundary --> log(prior) = 0 (uninformitive prior)\n",
    "    if 0.001<sigma<2 and 0.001<sigma_2<2 and -10<dg<-3 and -4<ddg<4 and -50<dh<0 and -40<ddh<40 \\\n",
    "        and (pt_true-pt_true*concrange_pt)<pt<(pt_true+pt_true*concrange_pt) and (lt_true-lt_true*concrange_lt)<lt<(lt_true+lt_true*concrange_lt) and -30<dh_0<30 \\\n",
    "        and (pt_true_2-pt_true_2*concrange_pt)<pt_2<(pt_true_2+pt_true_2*concrange_pt) and (lt_true_2-lt_true_2*concrange_lt)<lt_2<(lt_true_2+lt_true_2*concrange_lt) and -30<dh_0_2<30:\n",
    "        return 0  \n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_probability(theta, y_obs, extra_parameters):\n",
    "    '''log of estimated posterior probability'''\n",
    "    logp = log_prior(theta)\n",
    "    if not np.isfinite(logp):\n",
    "        return -np.inf  # ~zero probability\n",
    "    return logp + log_likelihood(theta, y_obs, extra_parameters)  # log posterior ~ log likelihood + log prior\n",
    "\n",
    "# initialization\n",
    "seed = 787878\n",
    "conc_priors = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "#filename for save and for plots\n",
    "filename = 'global_run1'\n",
    "\n",
    "\n",
    "#reading IC data\n",
    "extra_parameters_1,y_obs_1 = get_data('IC_into_NDL1_yeast_isotherm_1')\n",
    "extra_parameters_2,y_obs_2 = get_data('IC_into_NDL1_yeast_isotherm_2')\n",
    "\n",
    "#packaging is a bit weird for global model, this works but is clunky. Need to rewrite\n",
    "extra_parameters = extra_parameters_1 + extra_parameters_2\n",
    "y_obs = [y_obs_1,y_obs_2]\n",
    "\n",
    "print(extra_parameters,y_obs)\n",
    "\n",
    "#list ref concentrations\n",
    "pt_ref = 0.0004\n",
    "pt_ref_2 = 0.0004\n",
    "lt_ref = 1.2e-5 \n",
    "lt_ref_2 = 1.45e-5\n",
    "n_dim = 12\n",
    "\n",
    "\n",
    "\n",
    "# sampler settings\n",
    "n_walkers = 100  # at least 3x the number of parameters\n",
    "n_steps = int(3e5)  # at least 50x the autocorrelation time\n",
    "\n",
    "\n",
    "\n",
    "# Set up the backend -- emcee's built in stuff seems fairly legit\n",
    "# Don't forget to clear it in case the file already exists\n",
    "backend = emcee.backends.HDFBackend(f'save_{filename}.dat')\n",
    "backend.reset(n_walkers, n_dim)\n",
    "\n",
    "\n",
    "##limits on concs \n",
    "concrange_pt = 0.1\n",
    "concrange_lt = 0.1\n",
    "\n",
    "# random starts from uniform priors\n",
    "pos_list = []\n",
    "for i in range(n_walkers):\n",
    "    sigma_i = np.random.uniform(0.001,2)\n",
    "    sigma_2_i = np.random.uniform(0.001,2)\n",
    "    dg_i = np.random.uniform(-10,-3)\n",
    "    ddg_i = np.random.uniform(-4,4)\n",
    "    dh_i = np.random.uniform(-50,0)\n",
    "    ddh_i = np.random.uniform(-40,40)\n",
    "    pt_i = np.random.uniform(pt_ref-pt_ref*concrange_pt, pt_ref+pt_ref*concrange_pt)\n",
    "    pt_2_i = np.random.uniform(pt_ref_2-pt_ref_2*concrange_pt, pt_ref_2+pt_ref_2*concrange_pt)\n",
    "    lt_i = np.random.uniform(lt_ref-lt_ref*concrange_lt, lt_ref+lt_ref*concrange_lt)\n",
    "    lt_2_i = np.random.uniform(lt_ref_2-lt_ref_2*concrange_lt, lt_ref_2+lt_ref_2*concrange_lt)\n",
    "    dh_0_i = np.random.uniform(-15,-5)\n",
    "    dh_0_2_i = np.random.uniform(-25,-15)\n",
    "    pos_list.append([dg_i,ddg_i,dh_i,ddh_i,pt_i,pt_2_i,lt_i,lt_2_i,dh_0_i,dh_0_2_i,sigma_i,sigma_2_i])\n",
    "start_pos = np.asarray(pos_list)\n",
    "\n",
    "# run emcee ensemble sampler\n",
    "with Pool(processes=6) as pool:\n",
    "    sampler = emcee.EnsembleSampler(n_walkers, n_dim, log_probability,  args=(y_obs,extra_parameters),\n",
    "                                    backend=backend, pool=pool, \n",
    "                                    moves=[(emcee.moves.StretchMove(),0.8),\n",
    "                                            (emcee.moves.DEMove(),0),\n",
    "                                            (emcee.moves.DESnookerMove(),0.2)])\n",
    "    sampler.run_mcmc(start_pos, n_steps, progress=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 parameter itc model analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import corner \n",
    "\n",
    "#retrieve saved data (not always needed, comment out when unused)\n",
    "import emcee\n",
    "import numpy as np\n",
    "import itcfunctions as itc\n",
    "#parameter itc model analysis\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import corner\n",
    "\n",
    "\n",
    "#retrieve saved data (not always needed, comment out when unused) import emcee import numpy as np import itcfunctions as itc #filename = 'global_scratch'\n",
    "sampler = emcee.backends.HDFBackend(f'save_{filename}.dat') \n",
    "samples = sampler.get_chain(thin=100,discard=0)\n",
    "burn_in = 0\n",
    "n_dim = len(samples[0][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mcmc trajectory (before burn in)\n",
    "fig, axes = plt.subplots(n_dim, figsize=(10, 7), sharex=True)\n",
    "labels = [\"dg\", \"ddg\", \"dh\", \"ddh\", \"pt\", 'pt_2',\"lt\", 'lt_2',\"dh_0\",'dh_0_2', \"sigma\",'sigma_2']\n",
    "for i in range(n_dim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");\n",
    "plt.savefig(f'{filename}_traces.png')\n",
    "# autocorrelation time (for measuring +ampling performance)\n",
    "# note: this can give an error if the run length isn't 50x the autocorrelation\n",
    "#tau = sampler.get_autocorr_time()\n",
    "#print(tau)\n",
    "\n",
    "\n",
    "# corner plot (1d and 2d histograms)\n",
    "\n",
    "flat_samples = sampler.get_chain(discard=burn_in, thin=50, flat=True)\n",
    "n_v_list = [122,122,122,122,122,122,122]\n",
    "theta_true = [-7,-1,-10,-1.5, 0.0005, 1.7e-05, 0.2]\n",
    "bounds = [(-9,-5),(-4,4),(-20,0),(-10,10),(0.0005-0.0005*0.05,0.0005+0.0005*0.05), ((1.7e-05)-(1.7e-05)*0.05, (1.7e-05)+(1.7e-05)*0.05), (0.001,0.5)]\n",
    "auto = True\n",
    "\n",
    "if not auto:\n",
    "    fig = corner.corner(\n",
    "        flat_samples, labels=labels, truths=theta_true,\n",
    "        bins=n_v_list, range = bounds, plot_contours=True,\n",
    "        plot_density=True,\n",
    "    )\n",
    "else:\n",
    "    fig = corner.corner(\n",
    "        flat_samples, labels=labels,\n",
    "    )\n",
    "plt.savefig(f'{filename}_2dcorr.png')\n",
    "\n",
    "\n",
    "# plot y_predicted and y_observed\n",
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "plt.figure(figsize = (15, 10))\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    #print(sample)\n",
    "    y_pred_i_1 = itc.get_dq_list(sample[0],sample[1],sample[2],sample[3],sample[4], sample[6], sample[8],extra_parameters[0],itc_constants)\n",
    "    y_pred_i_2 = itc.get_dq_list(sample[0],sample[1],sample[2],sample[3],sample[5], sample[7], sample[9],extra_parameters[1],itc_constants)\n",
    "    y_pred_i = np.concatenate((y_pred_i_1,y_pred_i_2))\n",
    "    plt.plot(y_pred_i, alpha=0.1, color='grey')\n",
    "    \n",
    "plt.title('y_pred and y_obs')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "\n",
    "plt.plot(np.concatenate((y_obs[0],y_obs[1])), ls='None', color='black', marker='o',label=\"observed\")\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.savefig(f'{filename}_example_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b50d0253c9233ad55da087e51ac59a96e63f50c7917f518229b98d75afa1114b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
